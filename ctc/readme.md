## 端对端语音识别
什么叫端对端？
这个是一个相对的概念，相对于语言模型和语音模型两者分开的模型来说的。
端对端并不是输入语音输出文本，而是输入的是语音，而输出的可以是音素、可以是拼音、可以是汉字。
端 这个字的概念是说采用ctc损失函数，并不需要像传统的那样，每一帧语音都要对应上对应的标签，然后训练模型。
端对端模型只需要知道输入是一段音频，输出是对应的正确标签就行。

一般来说训练端对端语音识别模型的输出是拼音。在这个之后，还是需要做一个维特比的，来解码成真正的汉字，而做维特比的时候，需要知道字之间的转移概率，这个其实就是语言模型了，可以用2-gram或者n-gram
## deep speech

模型结构：1-d cnn  + 3 gru + ctc
其中：输入一般用fft或者mfcc
    然后会做padding。跟nlp是一样的。
    语音数据和nlp一样，在接入embedding后只能接1-d cnn，并且width为1，不能直接接2-d cnn，但是可以在1-d cnn的下一层接 2-d cnn。类似于deep qq sim中的arc、bcnn系列。
    1-d cnn 之后 接 3个stack gru，这个时候的输出仍然是多个向量。
    然后接dense层用softmax激活。意味着对每帧数据上做了多分类，而分类的标签是拼音或者音素或者汉字。
    然后接ctc，发现ctc其实只是一个损失函数了。。

解码：贪婪解码
优化方法：beam search解码。


